{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style TYPE=\\\"text/css\\\">\n",
    "    code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}\n",
    "    </style>\n",
    "    <script type=\\\"text/x-mathjax-config\\\">\n",
    "    MathJax.Hub.Config({\n",
    "        tex2jax: {\n",
    "            inlineMath: [['$','$'], ['\\\\\\\\(','\\\\\\\\)']],\n",
    "            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry\n",
    "    }\n",
    "    });\n",
    "    MathJax.Hub.Queue(function() {\n",
    "        var all = MathJax.Hub.getAllJax(), i;\n",
    "        for(i = 0; i < all.length; i += 1) {\n",
    "            all[i].SourceElement().parentNode.className += ' has-jax';\n",
    "        }\n",
    "    });\n",
    "    </script>\n",
    "    <script type=\\\"text/javascript\\\" src=\\\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\\\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Data\n",
    "\n",
    "Why do we need special models for these?\n",
    "\n",
    "### Motivation\n",
    "\n",
    "#### Language Modelling: Computing Probablity of a sentence\n",
    "\n",
    "* Text data\n",
    "\n",
    "\n",
    "    The dog jumped over the table.\n",
    "    The table jumped over the dog.\n",
    "\n",
    "Cons of other models when dealing with sequential data:\n",
    "\n",
    "* Feed forward networks with the entire sequence as input\n",
    "    1. Input size isn't fixed. Workaround? Append with special elements representing the end. But we also have huge input size: Lead to huge models.\n",
    "    2. Are we harnessing the sequential information present?\n",
    "    3. Parameters should learn to identify stuff at every place. Workaround? Convolutions! Some models do use CNNs for Sequences.\n",
    "\n",
    "\n",
    "* Feed forward networks processing one element of the sequence at a time.\n",
    "    1. Are we harnessing the sequential information present? NO! Infact treating everything as the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "* Way to harness the sequential information.\n",
    "\n",
    "\n",
    "* Simple addition to the second model. Just pass information from one time step to the other so as to help it understand better.\n",
    "\n",
    "\n",
    "* Downside: Loss of parallelization!\n",
    "\n",
    "\n",
    "\n",
    "<img src='./images/RNN.png' width=\"600\"></img>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Prop:\n",
    "\n",
    "<img src='./images/Prop.png' width=\"500\"></img>\n",
    "\n",
    "* $$ h^{<t>} = g ( W . h^{<t-1>} + U.x^{<t>} + b) $$\n",
    "\n",
    "\n",
    "$$ a^{<t>} = f ( V . h^{<t>} + c) $$\n",
    "\n",
    "$$ g: tanh \\quad f: softmax $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs and GRUs\n",
    "\n",
    "* Drawbacks of RNN: Backpropagation through time leads to Vanishing/Exploding Gradients.\n",
    "\n",
    "* Developed new models to handle these drawbacks\n",
    "\n",
    "<img src='./images/GRU_LSTM.jpg'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've quickly studied the building blocks for sequence models. Now let's look at applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK: Binary sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, LSTM, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the pizza was okay not the best ive had i prefer biaggios on flamingo  fort apache the chef there can make a much better ny style pizza the pizzeria  cosmo was over priced for the quality and lack of personality in the food biaggios is a much better pick if youre going for italian  family owned home made recipes people that actually care if you like their food you dont get that at a pizzeria in a casino i dont care what you say</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love this place my fiance and i go here atleast once a week the portions are huge food is amazing i love their carne asada they have great lunch specials leticia is super nice and cares about what you think of her restaurant you have to try their cheese enchiladas too the sauce is different and amazing</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terrible dry corn bread rib tips were all fat and mushy and had no flavor if you want bbq in this neighborhood go to john mulls roadkill grill trust me</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>back in 20052007 this place was my favorite thai place ever id go here alllll the time i never had any complaints once they started to get more known and got busy their service started to suck and their portion sizes got cut in half i have a huge problem with paying more for way less food the last time i went there i had the pork pad se ew and it tasted good but i finished my plate and was still hungry i used to know the manager here and she would greet me with a hello melissa nice to see you again diet coke  pad thai or pad se ew now a days i know she still knows me but she disregards my presence also i had asked her what was up with the new portion sizes and she had no answer for me great food but not worth the money i havent been back in over a year because i refuse to pay 1015 for dinner and still be hungry after sorry pinkaow you are not what you used to be</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delicious healthy food the steak is amazing fish and pork are awesome too service is above and beyond not a bad thing to say about this place worth every penny</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  the pizza was okay not the best ive had i prefer biaggios on flamingo  fort apache the chef there can make a much better ny style pizza the pizzeria  cosmo was over priced for the quality and lack of personality in the food biaggios is a much better pick if youre going for italian  family owned home made recipes people that actually care if you like their food you dont get that at a pizzeria in a casino i dont care what you say                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1  i love this place my fiance and i go here atleast once a week the portions are huge food is amazing i love their carne asada they have great lunch specials leticia is super nice and cares about what you think of her restaurant you have to try their cheese enchiladas too the sauce is different and amazing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "2  terrible dry corn bread rib tips were all fat and mushy and had no flavor if you want bbq in this neighborhood go to john mulls roadkill grill trust me                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "3  back in 20052007 this place was my favorite thai place ever id go here alllll the time i never had any complaints once they started to get more known and got busy their service started to suck and their portion sizes got cut in half i have a huge problem with paying more for way less food the last time i went there i had the pork pad se ew and it tasted good but i finished my plate and was still hungry i used to know the manager here and she would greet me with a hello melissa nice to see you again diet coke  pad thai or pad se ew now a days i know she still knows me but she disregards my presence also i had asked her what was up with the new portion sizes and she had no answer for me great food but not worth the money i havent been back in over a year because i refuse to pay 1015 for dinner and still be hungry after sorry pinkaow you are not what you used to be   \n",
       "4  delicious healthy food the steak is amazing fish and pork are awesome too service is above and beyond not a bad thing to say about this place worth every penny                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "   stars  sentiment  \n",
       "0  2      0          \n",
       "1  5      1          \n",
       "2  1      0          \n",
       "3  2      0          \n",
       "4  5      1          "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Dataset.csv')\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameters that can be tuned:\n",
    "    * num_words\n",
    "    * max_length_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31493 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "num_words = 5000\n",
    "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_length_of_text = 200\n",
    "X = pad_sequences(X, maxlen=max_length_of_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    3,   22,  269, 1570,\n",
       "          9,    4,  700,    7, 1610,  104,    1,  199,  694,  201,    2,\n",
       "         22,   23,   35,   33,  792,   39,   56,    8,    4,   33,    2,\n",
       "        140,  191,   25,   35,  465,   41,   56,  354,   21,   22,    4,\n",
       "        399,   11, 1536,   56,   58,  105,   10,    9,    4,   33,  147,\n",
       "         54,  165,   19, 1622,   11,    1, 1278,  877,   41,   19,  206,\n",
       "         10,  391], dtype=int32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[7653]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 200) (8000,)\n",
      "(2000, 200) (2000,)\n"
     ]
    }
   ],
   "source": [
    "y = data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 200, 50)           250000    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 341,777\n",
      "Trainable params: 341,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 50\n",
    "lstm_out = 128\n",
    "batch_size = 32\n",
    "\n",
    "inputs = Input((max_length_of_text, ))\n",
    "x = Embedding(num_words, embed_dim)(inputs)\n",
    "x = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "x = Dense(1,activation='sigmoid')(x)\n",
    "model = Model(inputs, x)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 125s 16ms/step - loss: 0.4711 - acc: 0.7738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2155dd0668>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = batch_size, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 12s 6ms/step\n",
      "Score: 0.37\n",
      "Validation Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, y_test, batch_size = batch_size)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Validation Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word vectors\n",
    "\n",
    "* What are word vectors and why use them?\n",
    "    * The size of one hot vectors becomes large if you have a lot of words in the vocabulary.\n",
    "    * One hot vectors treat every word to be equally dissimilar from each other, while this is not the case.\n",
    "    * The dop leaped over the fence v/s The dog jumped over the fence. Ideally we would want words with same meaning to have the same representation.  \n",
    "    * Word vectors are a way to encode the semantic meaning of a word.\n",
    "\n",
    "\n",
    "* How do we learn them?\n",
    "    * Representation Learning\n",
    "    * Skip-gram model, GloVe model\n",
    "\n",
    "* Game of Thrones characters:\n",
    "\n",
    "<img src='./images/WordEmbeddings.png'></img> \n",
    "\n",
    "* Word2vec Analogies:\n",
    "\n",
    " <img src='./images/WVecAnalogies.jpg' width=\"500\"></img> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can fine tune the embeddings to suit our job. Set trainable to True and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length_of_text,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 200, 100)          3149400   \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,266,777\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 3,149,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs2 = Input((length_of_text, ))\n",
    "x2 = embedding_layer(inputs2)\n",
    "x2 = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2, return_sequences=False)(x2)\n",
    "x2 = Dense(1,activation='sigmoid')(x2)\n",
    "model2 = Model(inputs2, x2)\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 104s 13ms/step - loss: 0.5714 - acc: 0.6993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2162936e10>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, batch_size = batch_size, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 10s 5ms/step\n",
      "Score: 0.45\n",
      "Validation Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "score,acc = model2.evaluate(X_test, y_test, batch_size = batch_size)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Validation Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is there a decrease in accuracy?\n",
    "\n",
    "* Sentiment Classification is fairly easy.\n",
    "\n",
    "* Number of parameters\n",
    "\n",
    "* Check the same for the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read about Word vectors impact of Machine Translation\n",
    "\n",
    "\n",
    "* Also see a document classification example:\n",
    "    * https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html (Using the news20.tar.gz dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASKS FOR TODAY'S LAB\n",
    "\n",
    "### Tune hyperparameters to achieve better accuracy\n",
    "\n",
    "### Predict the stars rating for a review using a similar models as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs2 = Input((length_of_text, ))\n",
    "x2 = embedding_layer(inputs2)\n",
    "x2 = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2, return_sequences=False)(x2)\n",
    "x2 = Dense(5,activation='softmax')(x2)\n",
    "model2 = Model(inputs2, x2)\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_gpu_tensorflow]",
   "language": "python",
   "name": "conda-env-keras_gpu_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
