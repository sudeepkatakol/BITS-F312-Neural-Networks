{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "labelled = unpickle('cifar-10-python/cifar-10-batches-py/data_batch_1')\n",
    "X_labelled = (np.reshape(labelled[b'data'], (-1, 3, 32, 32)).transpose(0, 2, 3, 1)).astype(np.uint8)\n",
    "y_labelled = np.array(labelled[b'labels'])\n",
    "\n",
    "X_unlabelled = []\n",
    "for i in range(2, 6):\n",
    "    print(i)\n",
    "    x1 = unpickle('cifar-10-python/cifar-10-batches-py/data_batch_'+str(i))\n",
    "    x2 = x1[b'data']\n",
    "    X_unlabelled = X_unlabelled + x2.tolist()\n",
    "X_unlabelled = np.array(X_unlabelled)\n",
    "X_unlabelled = np.reshape(X_unlabelled, (-1, 3, 32, 32))\n",
    "X_unlabelled = X_unlabelled.transpose(0, 2, 3, 1)\n",
    "X_unlabelled = X_unlabelled.astype(np.uint8)\n",
    "\n",
    "test = unpickle('cifar-10-python/cifar-10-batches-py/test_batch')\n",
    "X_test = (np.reshape(test[b'data'], (-1, 3, 32, 32)).transpose(0, 2, 3, 1)).astype(np.uint8)\n",
    "y_test = np.array(test[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 32, 32, 3), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_labelled.shape, y_labelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_unlabelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 32, 32, 3), (10000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, BatchNormalization, Input, Dense, Flatten\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 114,634\n",
      "Trainable params: 114,186\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def BaseModel():\n",
    "    X_input = Input((32, 32, 3))\n",
    "    X = Conv2D(32, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(64, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(128, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(10, activation=\"softmax\")(X)\n",
    "    return Model(X_input, X)\n",
    "base_model = BaseModel()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalise \n",
    "X_labelled_n = X_labelled / 255.\n",
    "X_test_n = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvalues = 10\n",
    "y_labelled_encoded = np.eye(nvalues)[y_labelled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Upgrade to Keras 2.2.3+\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 2.8015 - acc: 0.0950 - val_loss: 2.5904 - val_acc: 0.2000\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 509us/step - loss: 1.3430 - acc: 0.5350 - val_loss: 2.6089 - val_acc: 0.1800\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 575us/step - loss: 0.6886 - acc: 0.8400 - val_loss: 2.6157 - val_acc: 0.2200\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 580us/step - loss: 0.3351 - acc: 0.9650 - val_loss: 2.6052 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6b5df0d68>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.fit(X_labelled_n[:points], y_labelled_encoded[:points], epochs=10, batch_size=128, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 207us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.644049737930298, 0.1774]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.evaluate(X_test_n, np.eye(nvalues)[y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_labelled_n, X_test_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Autoencoders\n",
    "from keras.layers import Conv2DTranspose\n",
    "def Encoder():\n",
    "    X_input = Input((32, 32, 3))\n",
    "    X = Conv2D(32, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(64, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(128, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    return Model(X_input, X)\n",
    "\n",
    "def Decoder(encoder):\n",
    "    # Transpose Convolution\n",
    "    X = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(encoder.output)\n",
    "    X = Conv2DTranspose(32, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"SAME\")(X)\n",
    "    ## Sigmoid Activation\n",
    "    X = Conv2DTranspose(3, (3, 3), strides=(2, 2), activation=\"sigmoid\", padding=\"SAME\")(X)\n",
    "    return Model(encoder.input, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 64)          73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 32, 32, 3)         867       \n",
      "=================================================================\n",
      "Total params: 187,267\n",
      "Trainable params: 186,819\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder(encoder)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fetch normalised data\n",
    "def autoencoder_generator(X, BATCH_SIZE):\n",
    "    index = 0\n",
    "    while True:\n",
    "        if index + BATCH_SIZE <= len(X):\n",
    "            data = X[index:index+BATCH_SIZE]/255.\n",
    "            index += BATCH_SIZE\n",
    "        else:\n",
    "            np.random.shuffle(X)\n",
    "            data = X[:BATCH_SIZE]/255.\n",
    "            index = BATCH_SIZE\n",
    "        yield data, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([X_labelled, X_unlabelled], axis = 0)\n",
    "split = int(0.9*len(X))\n",
    "X_train = X[:split]\n",
    "X_val = X[split:]\n",
    "fetch = autoencoder_generator(X_train, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 32, 32, 3), (5000, 32, 32, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.0300 - val_loss: 0.0147\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 0.0079 - val_loss: 0.0071\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 0.0059 - val_loss: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6b00b1400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.fit_generator(fetch, steps_per_epoch=50, epochs=5, validation_data=(X_val/255., X_val/255.), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check generations\n",
    "reconstructions = decoder.predict(X_test[:5]/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "i = 0 ## Check for i = 0, 1, 2, 3, 4\n",
    "plt.imshow(X_test[i]/255.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAH3VJREFUeJztnWuMZVeV3//r3Hfdenf1u223MSYBJoxhOhYSyYgwyciDRjJImRF8QP6AxqPRIAVp5oNFpECkfGCiAOJDRNQEC09EeGQAYSUoGcaayBmN8ND2GL8asDFtu+3qqu7qetz3veeelQ91nbSb/T91u6vrVpv9/0mtun3W3Wfvs89Z99y7/2etZe4OIUR8JPs9ACHE/iDnFyJS5PxCRIqcX4hIkfMLESlyfiEiRc4vRKTI+YWIFDm/EJFS3E1jM7sHwBcBFAD8Z3f/bN77p+tVX5yvB23DlD9pWCpacLsbb1MYptSWZtSExMN9AUDG+nPe19AL1FYs5PVFTbCMH0BK+svrC7kmPsfDLOfeQZ4czXI6yzsut7y+uIntsWg55yznnlhK+Pg9x2bDAbWl7GRn/MDYPG62uuh0Bzln9P9z3c5vZgUA/xHAvwBwHsCPzOxhd3+OtVmcr+NP/+ieoK15mZ/4I0vhYaZJn7apb16itrUWP7lTzm2tEjkZnRXappEtUNuhGT79rRI/f0m3RW0bg5ng9qW5Cm2TlnMu9oxftK1Omdoy8mHeTPmHYanXprZBYYrakj6/djrkxrFQukjbbDnv60itRG2DKT4fla0L1LbaIO06/AOqXQjP49f+xxO0zdXs5mv/3QBecPcX3b0P4BsA7t3F/oQQE2Q3zn8cwCtX/P/8aJsQ4k3Abpw/9H3ql77rmdn9ZnbGzM40W91ddCeEuJHsxvnPA7jliv+fAPDa1W9y99PufsrdT03Xq7voTghxI9mN8/8IwJ1mdruZlQF8BMDDN2ZYQoi95rpX+909NbNPAPhf2Jb6HnT3Z/PaJJmj3g6v0M8W+crm4mud4PbuHF/tz7b4ynE95e0qDb5y3C+Gf7Z0BlySOZCzqrzQnqW26YM9amt1+Yp5IVkNbj+2eYC2adR4X50BX932tEFtSTt8XynmSF5bfX5cljSpbWi8Xb8bPrZ6zvU2W+Tns9bg81GrrfFxtPg1V+qHjy1t8mtxaOH9ec78Xs2udH53/z6A7+9mH0KI/UFP+AkRKXJ+ISJFzi9EpMj5hYgUOb8QkbKr1f5rxc0xLIUllmLC5ZXuVjhwpphx2QV9LvUVp/hhN17ZoLbabFhSKrW5VLZ0G5fzFgbrvK87F6lt47ktahsk4TkZrr9E2xQr89Q23+FyXq/Jg48utcJzUswLfmnza6AFHpjU7YSlYADoJOH721yZS28+w6+d1kXeV/EQf4itVOQSXMXIsTX53C/Wwn0Vr+F2rju/EJEi5xciUuT8QkSKnF+ISJHzCxEpE13th2dI++HV0sI6XzG/TIIwaht8BXUqnCoQAFBu83bFRf55OCBxFrX5Gm2z6DkBHUt8RX86J2dd+yDvL7sQDj7aqObkntvieRYudvgK/KDL57HZC09W0XOCVVJ+XOUan8etnDRe/WH4uFs5+QKLPD4HrYQHBJXX+Bhn63z+F8mcXM5RRgokJ+O13M115xciUuT8QkSKnF+ISJHzCxEpcn4hIkXOL0SkTFTqy9IM7fWwrNRY41VvCllY8hgmPADjwDwPBKlO88+8BDwQJ2W57lZ5gJEtcKksOTxHbcNyuPIOAKR9Xv3lYhau5vOjn3Npq0LaAMDP1vglUqly2StbPBLcfqzE575c5tJWv87Hb0TOA4DSIBwc09nIGXuVS5/NDT7+Ycqvx2MV3q54LDz+2Vk+H8XpsK1wDZE9uvMLESlyfiEiRc4vRKTI+YWIFDm/EJEi5xciUnYl9ZnZOQANAEMAqbufym3gGYoDIiuVeUTX4HJYrkmLPIoqzflcK8/wXGvOKz+hj7C80jrG5bz1Ad+hbw2pbTrl+3xlwNu9tB6WjTZzSlA1G9y2mXEbciIPD9bDkmlpaYG2WTyQk6evz+XUXpXLxOl6ePxlUu4KALqDHJnVeN7FRo/PVbmSE9VHTKUC9wmmAibG+7maG6Hz/zN3v3QD9iOEmCD62i9EpOzW+R3AX5rZ42Z2/40YkBBiMuz2a//73P01MzsE4Adm9hN3f/TKN4w+FO4HgIU6f1xRCDFZdnXnd/fXRn9XAXwXwN2B95x291PufqpenWzWMCEE57qd38zqZjbz+msAvw3gmRs1MCHE3rKbW/FhAN+1bWmhCOC/uvv/zGvghQL69XBpqOXmKm2XdMOyTI4IhTqvdIScyk9IG1xGG5TDtgsrPOIsneLRY50hP4LZ+ia19RGOmAOAVjk8lnSKS2xph0ejDXISkFareVlSwxGLhalDtEmtcpTakjIvo+ZbfP673fCxeTLN22R87i/lRM11BlxmS7r8urrcC8vBhzN+oc4shW1JMUervorrdn53fxHAr19veyHE/iKpT4hIkfMLESlyfiEiRc4vRKTI+YWIlMkm8MwcrT5Jgtnnn0OXiakOLvHklOPDRoPLb4M0J1IwDcsolzLeWb/Fx7jV4fLVpTZ/GnJx6UCO7Vhw+6DA5TzPidxr9vk4POHHVrLwpVWo8P0NajnnJWc+rLDFbZWwJJY2eNLSLONuMSA1CAGgl5OctAUuA272w8e91ueJRA/0w1Kl+/hRfbrzCxEpcn4hIkXOL0SkyPmFiBQ5vxCRMtHVfiuWUJgPB28cc56vbHoxvLpdHfLonVLO/tqs7BaA7pCvAns7PF0nb+XBKoVbF6mt1eP54KzJc9Yl3YPU1iOnNG3zvtDmx1zo89VtL/HAntX18LmplXnGt6UZXirNZpaobeZcWOEAgIutJ4PbCxUe2GOWU+qtys9LmvA5vrzOA3tmC+FV/VKVr9zbC1PB7b0cNeJqdOcXIlLk/EJEipxfiEiR8wsRKXJ+ISJFzi9EpExU6isWEhxaDJfKOjBzK23XK4UDTwZDLp+UGheprUUkOwBILvBSXslCWJK5/W6ezax+6O3Uttnl0lCnwSWb/maT2pbb4fxz3Q0eRLTOU9ahYWFJCQCG/ZzoqbWwbbPAz0v/rXweTxzg0u3Tt/Dzma2Fz2elwveXlPn+DtR4EFRymUtz1f5larto4eu4f4HvL/OwlNrL8Ymr0Z1fiEiR8wsRKXJ+ISJFzi9EpMj5hYgUOb8QkbKj1GdmDwL4XQCr7v5ro22LAL4J4CSAcwB+391zwsa2cS+gn4aj3DYyLlEcORKO9iqWuewyWOGSXWv1ArWtzXAZLWmExzGzdBttU5vnkWrFjNs2h1waWlufobZkJhzxNzPk+fEqpZzaZh1uy+a41FcmUYTVwe20TTXj56xU5pGMh2o8UvAXFs7913yVz0eZB2nC+lz6rBJJGgCaGY8G7HSIlG0587sRvm8P0rwidm9knDv/VwHcc9W2BwA84u53Anhk9H8hxJuIHZ3f3R8FcPVt6F4AD41ePwTgQzd4XEKIPeZ6f/MfdvdlABj9zfmiJIS4GdnzBT8zu9/MzpjZmWars9fdCSHG5Hqdf8XMjgLA6O8qe6O7n3b3U+5+arrOn6cWQkyW63X+hwHcN3p9H4Dv3ZjhCCEmxThS39cBvB/AkpmdB/BpAJ8F8C0z+ziAlwH83jidOQwZKXlVnMkpx2RhaataCu8LAC7nJOLc6nBJqdlbo7Zp0l/zVS7LFWdz+mry6d/o81A7X+LJJ+eG4TEunuDz28YctTXXuPSZkmg0AJhZCidqnVnkbQ4f598MjyzyZKGvvsjlrYV6eI5Xerx8WSHjZbIS42OsznIZcEC/GwOWhaP3ikSmBID+MHzfHl/oG8P53f2jxPRb19CPEOImQ0/4CREpcn4hIkXOL0SkyPmFiBQ5vxCRMtEEnokZyqVwdJMjJ3FmIRz9Nki4bFTu8SiqXspltOIKj/YalMO2Z/9PuB4cAOBnL1BT0jvOx8HzmeLkcR7V53Nh2/SdvNZdLeVSZalxhNqSAZcBO/3w+ZxeOEHbzPf5+Sxc2qK29cbL1LZ1IRzB2d3i53muyGXFhcM8qaZPcWlusMTPWasXrh1ZzPi9eXY2LOkWCuPfz3XnFyJS5PxCRIqcX4hIkfMLESlyfiEiRc4vRKRMVOpzONJCP2irFPlQKhauWze0Cm3TGvCkju1LvA5eMsUloF4Wtv39uZ/QNuUWlxynFrh89Z6MR9qZv4XvsxqWorzEpdTDizxSbf0wj47sNri05bVjwe0HDvI2ac75vARea3DQ4BF6DQ+fs2LG499adS5hHq5xGXB2lsuARqRPAGh0w7JdkuOe9Wp4/MUCH8Mv718IESVyfiEiRc4vRKTI+YWIFDm/EJEy0dV+Q4ISwnnObHaetuuQrL8zxnOmFSs8kCKt8jx9zcs5K6zl8IrtRpmviM8OeL69pZzyTtX6HdQ2X+RqRbcbDhIZbvESZRuXeKW13gZXP7zDV9kXBi8Ft88d+Ye0TWmRz0e7Ez4uAFiqHKC26lT4Gtnc4MFdx40rLaUav+bmSX5KAHC+2I9KFlYySim/N5frYWWkmCiwRwixA3J+ISJFzi9EpMj5hYgUOb8QkSLnFyJSxinX9SCA3wWw6u6/Ntr2GQB/AODi6G2fcvfv77QvN0NWCksU1eGAD7IfDo7JKuEgIQDY6vOSS0Pn1YJLVR5cUl0ISzm3JTwwZnaJB7Lc8rZ3UduRW8PlrgCgOp2jG202gptf2uCyXLPN52q9wyWxrMJz7qVTYem20udBMycHPHhnhsisAJDMcBlwYTYstWZ93qZV5raDAz6P7TkuAxZIcBoAVIvhdoUBPy/lYnjubfy4nrHu/F8FcE9g+xfc/a7Rvx0dXwhxc7Gj87v7owB4elchxJuS3fzm/4SZPWVmD5rZwg0bkRBiIlyv838JwB0A7gKwDOBz7I1mdr+ZnTGzM80WfwxWCDFZrsv53X3F3YfungH4MoC7c9572t1Pufup6TrPgiKEmCzX5fxmduVS9IcBPHNjhiOEmBTjSH1fB/B+AEtmdh7ApwG838zuAuAAzgH4w7E6S0qYmwqXf6pUea676Xq4XJfnlFWamuFRcZV6TvSVc/mqWg23q828StvUi7xM1vzKOWprdfg4Vmtc4mz3wp/nT//0PO9rlctX03wYyHCI2hbnF4PbDy3waMuZPpdg2z3+k3HrlVVq67wWjuC0DR7ZWatx6Xalz/MuDjJe2mwB/Jor10jprYxHhGYZydWI8bW+HZ3f3T8a2PyVsXsQQtyU6Ak/ISJFzi9EpMj5hYgUOb8QkSLnFyJSJluuqwAMiczWm+bRdJ6Epb5BjqpRqPKkjqVSOLkkABSm+DjahXC018DD4wOAyynXytY3ecLKZPXH1Nav8NO22Q1HEVqLy4ODEo+msxypMs0pbVZbCMtlxYQf80sX+TjWOjy8ZGuFl2bb6ISjHPttHmXXLPJIxsPDsIQJANMJjwYcDHlS0GGZlLAz3peXwtLnjY7qE0L8CiLnFyJS5PxCRIqcX4hIkfMLESlyfiEiZaJSHyyBVUkixiqv1ZfMhodZaHOpaSonKi5LuezSXOeyUd/C/R1JeXLJ5Ravkfez53kyy1oxJ8no0mFqO7wUjgRbINIbABSKt1Kb5SSsHA64fNhrXQxuX9viWlShyO9Fr6z8gtqeXeYS4eVBeB6r4MkxD/b4+ZzKScR5dJ5fVzWSpBMA2oWwPNvr80jAnod9wq4hqk93fiEiRc4vRKTI+YWIFDm/EJEi5xciUia62m8wJCSXmfGqVkhTsupZ5EEil3mqOPTLfCXdSZAFACQksGezxT9DC7N8jI16OOgEAJrOV9n/wYAH2yycDPd34shdtE19nl8Gwws8yOXljZepLbsYDsRZzskJuLb8IrWtN/g5Wx9y1WTQD69+J6WcS3+Wr5jP1viFVZzlF3G3ylf7i2n4nHmZT1anGb7mriWHn+78QkSKnF+ISJHzCxEpcn4hIkXOL0SkyPmFiJRxynXdAuDPARwBkAE47e5fNLNFAN8EcBLbJbt+393X8/aVZYZ+OyyHzLW4RNGZDn9GJeABGL0tXt4p7XBJxnOSoB2afmtw+zvuOhrcDgCtnJyAxfbfUtszr56lttJhHthzy+I/DW4/+c530jYLc/yY/8p4YNLgIpeifrgcljEHOSW5zq/y82J5QUR9Pv/l6iC4vZXxoJnNnACj8+XbqG164wS1refkDKyVw9fqpUJOia9e+Lj62fjq/Th3/hTAn7j72wG8F8Afm9k7ADwA4BF3vxPAI6P/CyHeJOzo/O6+7O5PjF43AJwFcBzAvQAeGr3tIQAf2qtBCiFuPNf0m9/MTgJ4N4DHABx292Vg+wMCyCnZKoS46Rjb+c1sGsC3AXzS3Xmd4l9ud7+ZnTGzM83G2M2EEHvMWM5vZiVsO/7X3P07o80rZnZ0ZD8KIFgk3d1Pu/spdz81PcOLWwghJsuOzm9mBuArAM66++evMD0M4L7R6/sAfO/GD08IsVeMowu8D8DHADxtZk+Otn0KwGcBfMvMPg7gZQC/t/OuHEMPR8at9/nn0IFmuM2wmBONVuPRdAdK1IQT77qD2srzB4Pb3/6PP0DbWJHLaJ7w6LznvstlzOot76a27olwLsTlLj9on+FzPyR5CwHg520+xq6Hj3t5k0cyNpzvr8xNKC/wSLt6MSyXeZ3njDSe7hCdw7yE1tkqH0cnRwUfNMP5BMvO91e1cK7G1Lk8eDU7Or+7/w1A4wR/a+yehBA3FXrCT4hIkfMLESlyfiEiRc4vRKTI+YWIlIkm8EwHfayvng/aSl1eMqo0HU4GWVrgskuvyKPAyid4ZNahO3iixaXjbwtuP7C0QNv4kEuO04s8iq2yGJYVAeCZv+WJM5sr4XJjfht/uvLwNJ+rZ9aCz24BAFY3eJmsXkb2WeFPgVdtjdqKlQPUVp4Ny14AUK6Gdbue8WjL3hS/BoYdnlh1iQcsYrPLJd9hMfzwWyHl45ifDkcJeqIEnkKIHZDzCxEpcn4hIkXOL0SkyPmFiBQ5vxCRMlGpb5g5tlrhxIOV9CXa7lJ2LLi9nvA6clNbXP4pzfBkitU5LjnWa+F9liv8M7TZC0ckAsCrDR4x59lFanulxiWxzcvhqK7uxnO0zaFpHgm2ssHlpqTAJcLOMJxUs7rI2ww7PJyuUuJzVZ05Qm2zc+Hxr2fh6xAABptcnu2VuERoFX5szRypb6pEIg9r/JjNdn/f1p1fiEiR8wsRKXJ+ISJFzi9EpMj5hYiUia72t/uOx8+FV1nTGg8SKT0VjpiYX+CrwxudcF40ADj0Tp4D723149RWnQrn3HvqZb4q+/S516jtq985R20Xn/8ptXX9JLU1K82wweu0zaDK575R40rAIOV5AZNaOGipt87Vj47zoJ9Bl6+k951fxoNeOPnfpSLPn1juhQPJAKDf58pCt8Xz9JnzzNXFaniMSZXPfR/huXdwNeKX9j/2O4UQv1LI+YWIFDm/EJEi5xciUuT8QkSKnF+ISNlR6jOzWwD8OYAjADIAp939i2b2GQB/AOD1CJRPufv38/aVeYbOMCzbrS3z3GjZpbD08solPvzS0bdQW73JgzqaRS6VXErC8sr5Jpd4Hv1hOGchADSbT1Nbp8PLWtkSD1oakCCdSsoDS7YGvCxUWrmd91VuUZtl4XOTGa+71ZvhORmdlHkDgL4Nqa3bD5/rfsbve2k5xy1SLiH3ilxOtYU5aiuVwmOpFHhQVasSllKHhfHV+3HemQL4E3d/wsxmADxuZj8Y2b7g7v9h7N6EEDcN49TqWwawPHrdMLOzAPiTMEKINwXX9JvfzE4CeDeAx0abPmFmT5nZg2bG81cLIW46xnZ+M5sG8G0An3T3LQBfAnAHgLuw/c3gc6Td/WZ2xszODHr8d70QYrKM5fxmVsK243/N3b8DAO6+4u5Dd88AfBnA3aG27n7a3U+5+6lShS9gCCEmy47Ob2YG4CsAzrr756/YfmWepg8DeObGD08IsVeMs9r/PgAfA/C0mT052vYpAB81s7sAOIBzAP5wpx25O9JuWHopcAUFgzT8cyFr8qi++V/wMlPJ7b9Bbc01HknVnQtLSj9Z4W0uX+B5Bgfr89Q2U7+F2rJ53m6hfzi4vTnH5c0yKfEFAL02lz7LRS45dofh+0rJeZRjecBzKyaFnCWlNCfXXTEc5VhocHkwmeayXC8ngnAqR56tz/HSbBlRMft1fl2hG5bMPeNzeDXjrPb/DYCQSJyr6Qshbm70hJ8QkSLnFyJS5PxCRIqcX4hIkfMLESkTTeBZADBL1Is0p4RWeyssvRSLXEbbqv8jajs2JEkuAbzc5Ikipxth22srvLTWpfUL1Faa5fJPuXQbtfVqPDKuMxWWP4cNLuehzOc+9S1q237oM4yn4bkq5shX/ZzLsVLltjTl+ywlYYmzWwhLZQBgzh9GqxmPLqzN83HkBJKiSpJu9rtc/y44kTedlxq7Gt35hYgUOb8QkSLnFyJS5PxCRIqcX4hIkfMLESkTlfqGDqz3w4kkhwmXjcq1sHw1GHApZKvxKrU9f5ZLZesbPGLuxd8Ij+PxH67x/eVEj3nKxzFMuPxW7vEaeXVi2kyOhg0A0gqPiiv3eeTkVIWPv8rkyJyklNVuzjjaXJpLcySxZjUc3ZmkB2mb+kHuFtU5bnPwa7jS5pJprx2Wgzvn+bXTmg7Ly4MBn6er0Z1fiEiR8wsRKXJ+ISJFzi9EpMj5hYgUOb8QkTJRqS+xBFPlcD2zS7UcSYyERKXG5Zos40kYL7Z5cs/GSzyq72IrHLV1+dwLtE0v4xF/NuTym+dINr0ar603ILJd2uPRXmXn0YX9Gq/x15/nEttUcmtweyWYDnKbrYQn9xwO+DlzHqSJ6mY4+Wu6yMPsauEcqACAek5i1fIUl/ouvsqTW3cvhs911uJRq5XNleB2G+SED16F7vxCRIqcX4hIkfMLESlyfiEiRc4vRKTsuNpvZlUAjwKojN7/F+7+aTO7HcA3ACwCeALAx9ydL5UDMBiK5PNmhuTHA4BWeTE8+MI6bdNfWaa2bI4HkAxb3Nb2VnD7oMFXsPM+Xj07z20Frn6gx+eqPwyPJW1z9QNT4ZVjAJjafCu1JTy+CDYTzktXnOYBLtUmDxTqZTm56cp8PgYWnscTC7z8VynhtoUDXGEqFMPXBwBs5QT9tJPwqn6SXaZtBlPh4/Lkxubw6wH4gLv/OrbLcd9jZu8F8GcAvuDudwJYB/DxsXsVQuw7Ozq/b/O6kloa/XMAHwDwF6PtDwH40J6MUAixJ4z1m9/MCqMKvasAfgDg5wA23P9f/uDzAI7vzRCFEHvBWM7v7kN3vwvACQB3A3h76G2htmZ2v5mdMbMz/R7/TSSEmCzXtNrv7hsA/jeA9wKYN7PXFwxPAAg+m+nup939lLufKlfCj/YKISbPjs5vZgfNbH70ugbgnwM4C+CvAfzL0dvuA/C9vRqkEOLGM05gz1EAD5lZAdsfFt9y9/9uZs8B+IaZ/TsAfw/gKzvtKCkA0/NhKSLrcbms1w5HbhSKOWWacnLPFXOOOi3zgJpKEpaUslJO6ach7yzLuLRVqTeobXOLB9RUW+G5Svs8SKRa4n116lz2qlZ5YFKpFO5vaobrg+02lzerBT5X1uPt6iUifeaU1pqZ57Jczbj8VqyE5U0AqBqf/xrJhWhTOfkOi+Fjvmw5svNV7Oj87v4UgHcHtr+I7d//Qog3IXrCT4hIkfMLESlyfiEiRc4vRKTI+YWIFHMfPwpo152ZXQTw0ui/SwAuTaxzjsbxRjSON/JmG8dt7s5DD69gos7/ho7Nzrj7qX3pXOPQODQOfe0XIlbk/EJEyn46/+l97PtKNI43onG8kV/Zcezbb34hxP6ir/1CRMq+OL+Z3WNmPzWzF8zsgf0Yw2gc58zsaTN70szOTLDfB81s1cyeuWLbopn9wMyeH/3l4XR7O47PmNmrozl50sw+OIFx3GJmf21mZ83sWTP7V6PtE52TnHFMdE7MrGpmf2dmPx6N49+Ott9uZo+N5uObZsbrrI2Du0/0H4ACttOAvQVAGcCPAbxj0uMYjeUcgKV96Pc3AbwHwDNXbPv3AB4YvX4AwJ/t0zg+A+BPJzwfRwG8Z/R6BsDPALxj0nOSM46JzgkAAzA9el0C8Bi2E+h8C8BHRtv/E4A/2k0/+3HnvxvAC+7+om+n+v4GgHv3YRz7hrs/CuDqwPB7sZ0IFZhQQlQyjonj7svu/sTodQPbyWKOY8JzkjOOieLb7HnS3P1w/uMAXrni//uZ/NMB/KWZPW5m9+/TGF7nsLsvA9sXIYBD+ziWT5jZU6OfBXv+8+NKzOwktvNHPIZ9nJOrxgFMeE4mkTR3P5w/lGpkvySH97n7ewD8DoA/NrPf3Kdx3Ex8CcAd2K7RsAzgc5Pq2MymAXwbwCfdnVf3mPw4Jj4nvoukueOyH85/HsCVRc5p8s+9xt1fG/1dBfBd7G9mohUzOwoAo7+8IP0e4u4rowsvA/BlTGhOzKyEbYf7mrt/Z7R54nMSGsd+zcmo72tOmjsu++H8PwJw52jlsgzgIwAenvQgzKxuZjOvvwbw2wCeyW+1pzyM7USowD4mRH3d2UZ8GBOYEzMzbOeAPOvun7/CNNE5YeOY9JxMLGnupFYwr1rN/CC2V1J/DuBf79MY3oJtpeHHAJ6d5DgAfB3bXx8H2P4m9HEABwA8AuD50d/FfRrHfwHwNICnsO18Rycwjn+C7a+wTwF4cvTvg5Oek5xxTHROALwL20lxn8L2B82/ueKa/TsALwD4bwAqu+lHT/gJESl6wk+ISJHzCxEpcn4hIkXOL0SkyPmFiBQ5vxCRIucXIlLk/EJEyv8FhX17Sll5yp8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(reconstructions[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder.save('encoder.h5')\n",
    "## Unsupervised learning all this while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudeep/.conda/envs/keras_gpu_tensorflow/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "def AutoEncoderSemiSupModel(encoder):\n",
    "    X = Flatten()(encoder.output)\n",
    "    X = Dense(10, activation=\"softmax\")(X)\n",
    "    return Model(encoder.input, X)\n",
    "autoencsemisup1 = AutoEncoderSemiSupModel(load_model(\"encoder.h5\"))\n",
    "autoencsemisup2 = AutoEncoderSemiSupModel(load_model(\"encoder.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_labelled_n = X_labelled / 255.\n",
    "X_test_n = X_test / 255.\n",
    "nvalues = 10\n",
    "y_labelled_encoded = np.eye(nvalues)[y_labelled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7fc65cc2e8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc65cc2e978>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc65cc2ed68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc65cc2ec18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc65cc2ee80>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc65cc2efd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc65cc24278>,\n",
       " <keras.layers.core.Flatten at 0x7fc65c9ca9b0>,\n",
       " <keras.layers.core.Dense at 0x7fc65e79cdd8>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fix all the layers in the model\n",
    "autoencsemisup2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[autoencsemisup2.layers[i].trainable for i in range(len(autoencsemisup2.layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(autoencsemisup2.layers[:-2])):\n",
    "    autoencsemisup2.layers[i].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 114,634\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 94,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencsemisup2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "autoencsemisup2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.1685 - acc: 0.1100 - val_loss: 2.8910 - val_acc: 0.2600\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 208us/step - loss: 2.4480 - acc: 0.1800 - val_loss: 2.5982 - val_acc: 0.3400\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 284us/step - loss: 1.9253 - acc: 0.3150 - val_loss: 2.4505 - val_acc: 0.3400\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 260us/step - loss: 1.5540 - acc: 0.4600 - val_loss: 2.4190 - val_acc: 0.3400\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 250us/step - loss: 1.2710 - acc: 0.5800 - val_loss: 2.4756 - val_acc: 0.3600\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 270us/step - loss: 1.0500 - acc: 0.6700 - val_loss: 2.5594 - val_acc: 0.3600\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 308us/step - loss: 0.8931 - acc: 0.7250 - val_loss: 2.6437 - val_acc: 0.3600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc65ca0ce80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencsemisup2.fit(X_labelled_n[:points], y_labelled_encoded[:points], epochs=10, batch_size=128, validation_split=0.20, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 210us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.7939420974731446, 0.2074]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencsemisup2.evaluate(X_test_n, np.eye(nvalues)[y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 3.0859 - acc: 0.1000 - val_loss: 2.4718 - val_acc: 0.1600\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 475us/step - loss: 1.9198 - acc: 0.3250 - val_loss: 2.2121 - val_acc: 0.2600\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 489us/step - loss: 1.2664 - acc: 0.6150 - val_loss: 2.1742 - val_acc: 0.2800\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 668us/step - loss: 0.8420 - acc: 0.7750 - val_loss: 2.2463 - val_acc: 0.2400\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 670us/step - loss: 0.5880 - acc: 0.8650 - val_loss: 2.3309 - val_acc: 0.2400\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 641us/step - loss: 0.4005 - acc: 0.9350 - val_loss: 2.4264 - val_acc: 0.2600\n",
      "10000/10000 [==============================] - 2s 224us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5251216514587402, 0.2078]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencsemisup1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "autoencsemisup1.fit(X_labelled_n[:points], y_labelled_encoded[:points], epochs=10, batch_size=128, validation_split=0.2, callbacks=[callback])\n",
    "autoencsemisup1.evaluate(X_test_n, np.eye(nvalues)[y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 915,018\n",
      "Trainable params: 525,706\n",
      "Non-trainable params: 389,312\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudeep/.conda/envs/keras_gpu_tensorflow/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "def AutoEncoderSemiSupModel2(encoder):\n",
    "    #X = Flatten()(encoder.output)\n",
    "    X = Conv2D(256, (3, 3), activation=\"relu\", padding=\"SAME\")(encoder.output)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, activation=\"relu\")(X)\n",
    "    X = Dense(10, activation=\"softmax\")(X)\n",
    "    return Model(encoder.input, X)\n",
    "autoencsemisup = AutoEncoderSemiSupModel2(load_model(\"encoder.h5\"))\n",
    "for i in range(len(autoencsemisup.layers[:-3])):\n",
    "    autoencsemisup.layers[i].trainable = False\n",
    "autoencsemisup.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "autoencsemisup.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.4796 - acc: 0.1300 - val_loss: 2.1871 - val_acc: 0.1600\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 349us/step - loss: 1.7651 - acc: 0.4450 - val_loss: 2.1810 - val_acc: 0.2000\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 363us/step - loss: 1.2346 - acc: 0.6900 - val_loss: 2.2127 - val_acc: 0.2200\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 470us/step - loss: 0.9161 - acc: 0.7550 - val_loss: 2.3142 - val_acc: 0.2800\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 445us/step - loss: 0.6051 - acc: 0.9100 - val_loss: 2.4649 - val_acc: 0.2800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc65ca0cf98>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencsemisup.fit(X_labelled_n[:points], y_labelled_encoded[:points], epochs=10, batch_size=128, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 216us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5251216514587402, 0.2078]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencsemisup1.evaluate(X_test_n, np.eye(nvalues)[y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASKS\n",
    "\n",
    "* Can use unlabelled test data for training autoencoder! Try and reason whether or not this helps\n",
    "\n",
    "\n",
    "* Increase the points variable: 250 -> 1000 -> 5000 -> 10000. Check how extra labelled data impacts performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudeep/.conda/envs/keras_gpu_tensorflow/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE()\n",
    "enc = load_model('encoder.h5')\n",
    "codes = enc.predict(X_test_n[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2048)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes = np.reshape(codes, [-1, 4*4*128])\n",
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "projections = tsne.fit_transform(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource, reset_output, output_notebook\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.palettes import d3\n",
    "from bokeh.models import CategoricalColorMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "CATEGORY_DICT = OrderedDict({\n",
    "0 : \"airplane\", \n",
    "1: \"automobile\",\n",
    "2: \"bird\",\n",
    "3:\"cat\",\n",
    "4:\"deer\",\n",
    "5:\"dog\",\n",
    "6:\"frog\",\n",
    "7:\"horse\",\n",
    "8:\"ship\",\n",
    "9:\"truck\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_output()\n",
    "source = ColumnDataSource(data=dict(\n",
    "            x=projections[:,0],\n",
    "            y=projections[:,1],\n",
    "            labels = [CATEGORY_DICT[i] for i in y_test[:1000]]\n",
    "        ))\n",
    "color_map = CategoricalColorMapper(factors=list(CATEGORY_DICT.values()),\n",
    "                                       palette=d3['Category10'][10])\n",
    "color = {'field': 'labels', 'transform': color_map}\n",
    "legend = 'labels'\n",
    "p = figure(plot_width=1600, plot_height=800,title=\"TSNE_Projections\")\n",
    "p.circle('x', 'y', source=source, color=color, size=10, legend=legend)\n",
    "output_file(\"autoencoder_tsne.html\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK:\n",
    "\n",
    "* Train autoencoder to convergence and then visualise. Use 3-D TSNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-a37a59f11b0d>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/sudeep/.conda/envs/keras_gpu_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/sudeep/.conda/envs/keras_gpu_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sudeep/.conda/envs/keras_gpu_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sudeep/.conda/envs/keras_gpu_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    real_input = tf.placeholder(tf.float32, (None, real_dim))\n",
    "    z_input = tf.placeholder(tf.float32, (None, z_dim))\n",
    "    return real_input, z_input\n",
    "\n",
    "def generator(z, out_dim, n_units=128, reuse=False,  alpha=0.01):    \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        h1 = tf.layers.dense(z, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(h1, alpha*h1)\n",
    "        # Logits and tanh output\n",
    "        logits = tf.layers.dense(h1, out_dim, activation=None)\n",
    "        out = tf.nn.tanh(logits)\n",
    "        return out, logits\n",
    "\n",
    "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(x, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(h1, alpha*h1)\n",
    "        logits = tf.layers.dense(h1, 1, activation=None)\n",
    "        out = tf.nn.sigmoid(logits)\n",
    "        return out, logits\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 784 # 28x28 MNIST images flattened\n",
    "z_size = 100 # Size of latent vector to generator\n",
    "g_hidden_size = 128 # Size of hidden layer in generator\n",
    "d_hidden_size = 128 # Size of hidden layer in discriminator\n",
    "# Leak factor for leaky ReLU\n",
    "alpha = 0.01\n",
    "\n",
    "tf.reset_default_graph() \n",
    "# Create our input placeholders \n",
    "real_input, z = model_inputs(input_size, z_size)  \n",
    "# Generator network here \n",
    "generations, generation_logits = generator(z, input_size, g_hidden_size, reuse=False,  alpha=alpha) \n",
    "# g_model is the generator output  \n",
    "\n",
    "# Disriminator network here \n",
    "d_real, d_real_logits = discriminator(real_input, d_hidden_size, reuse=False, alpha=alpha) \n",
    "d_fake, d_fake_logits = discriminator(generations, d_hidden_size, reuse=True, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_labels_real = tf.ones_like(d_real)\n",
    "d_labels_fake = tf.zeros_like(d_fake)\n",
    "\n",
    "d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(labels=d_labels_real, logits=d_real_logits)\n",
    "d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(labels=d_labels_fake, logits=d_fake_logits)\n",
    "\n",
    "d_loss = tf.reduce_mean(d_loss_real + d_loss_fake)\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_fake_logits), logits=d_fake_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Get the trainable_variables, split into G and D parts\n",
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n",
    "d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer().minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "samples = []\n",
    "losses = []\n",
    "saver = tf.train.Saver(var_list = g_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(mnist.train.num_examples//batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Get images, reshape and rescale to pass to D\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images*2 - 1\n",
    "            \n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            \n",
    "            # Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={real_input: batch_images, z: batch_z})\n",
    "            _ = sess.run(g_train_opt, feed_dict={z: batch_z})\n",
    "        \n",
    "        # At the end of each epoch, get the losses and print them out\n",
    "        train_loss_d = sess.run(d_loss, {z: batch_z, real_input: batch_images})\n",
    "        train_loss_g = g_loss.eval({z: batch_z})\n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g))    \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(\n",
    "                       generator(z, input_size, reuse=True),\n",
    "                       feed_dict={z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_images = np.reshape(samples[-1][0], (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc08a5f65c0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFBpJREFUeJzt3W1wXOV1B/D/2dWbZctg2bEQtmMbMCTgtJCoJrwM45ZAICFjmCEEJ6VOy2CawjQ0yaQMX0I/pHVfEofpABNRPDGE8FZCoBkmgXHDEChQZEKwqWNsjA22hIRtLPkFy9rd0w9aMwL0nGe1d3fvivP/zXgs7dHd+2i1f91dnXufR1QVRORPJu0BEFE6GH4ipxh+IqcYfiKnGH4ipxh+IqcYfiKnGH4ipxh+IqcaarmzJmnWFkwN1qUha26vuXylh0T0kXIYB3FEh6WUr00UfhG5CMAtALIA/kNVV1lf34KpOFPOD9azM2aa+8vv3lPGKGtAIo917BTqpNtT5cV+JjFJfmYJng/P67qSd1P2y34RyQK4FcDFAE4FsFxETi33/oiotpK8518CYKuqblPVIwDuA7CsMsMiompLEv45AN4c8/nO4m3vIyIrRaRHRHpGMJxgd0RUSUnCP94bkw+9GVHVblXtUtWuRjQn2B0RVVKS8O8EMG/M53MB9CYbDhHVSpLwvwBgkYgsFJEmAFcCeLQywyKiaiu71aeqORG5HsCvMdrqW6OqryQZTLSVZ7VAqtwOk4bwQ6W5XKL7zrbPMOv5PXsT3X8isbaTRI4fhfC5GdLYZG6qI0fs+/6oqlFrN1GfX1UfA/BYhcZCRDXE03uJnGL4iZxi+ImcYviJnGL4iZxi+Imcqun1/IkZ/c9MW5u5aWH//mS7TtjLt8T6+NY5BkB1xxbtOWv5cyxE+/jVvNQ5Y88dYZ2fUIroOQy5EaOY4BLwCTwkPPITOcXwEznF8BM5xfATOcXwEznF8BM5NblafYbCwUPJ7qCabaWE952olVflllY1ZaZNM+vR9q31uCf9viOPa+I2pnnnlbnkl0d+IqcYfiKnGH4ipxh+IqcYfiKnGH4ipxh+IqcmV5+/mn3bSO800dTd1V6l1+o5xx6XSL/63S99xqzP/s42s/7v838RrL1dsJ9+Lx2ea9Yf2/NHZn1oWfhxLQza5whE+/Sxx3USrLzMIz+RUww/kVMMP5FTDD+RUww/kVMMP5FTDD+RU4n6/CKyHcB+AHkAOVXtSjSaOu6NSlN4KmaZMsXcVo/YPWMdHrZ3HunFN3R2BGuvrZ5pbntn11qz3tX8nL1v2GPLSvia/FmRab8fGgx/XwDw9Y5nzPoNt10RrJ34d/Zy8LldvWa9np+rparEST5/qqq7K3A/RFRDfNlP5FTS8CuAx0VkvYisrMSAiKg2kr7sP0dVe0VkNoAnROQPqvrU2C8o/lJYCQAtaE24OyKqlERHflXtLf4/AOBhAEvG+ZpuVe1S1a5GNCfZHRFVUNnhF5GpItJ29GMAFwLYWKmBEVF1JXnZ3wHgYRlteTQA+Jmq/qoioyKiqis7/Kq6DcAfT2gjSXZdfMOc44O1aF82Ijt9ulkvHAqvCxC9nj82d36kZywZu/7WJfODtZ6zV5vbTsu0mPXYi8N38vZ6CXsLhWDtr7cut7d9wL6ef92ec826nhEee27XBnPbqEnQx49hq4/IKYafyCmGn8gphp/IKYafyCmGn8ip2k7drXZbzGoDAsnaedIYviQXAPJDQ2a9YWG4nZZ7fYe984TTisvik836p//q5WCtUew2Y17DrTgAePCAfUnwtuFTzPrTly8O1jJbt5vbzm5626wf+ILdaT7ptvDPJcGi5x8ZPPITOcXwEznF8BM5xfATOcXwEznF8BM5xfATOVVXS3RHL41NIDN/jr3v3n6zHu3lJxG5pPed0+zLjb/R/rtgbX/Bnjb88k1fM+vZVXafv/Gp35t1wHjcIuc/qNpPz5Y9kWW0GyKXUjvHIz+RUww/kVMMP5FTDD+RUww/kVMMP5FTDD+RU3XV549OYZ0N921j5wjkX9tu7zvN5b8bGs36oePs39Gfm7IvWOvP299X61f3m/X8Hvv8Bo08btIcXqWp75sfWuDpfTqfOWDWC9nIsWvYOA/gI7DEdlI88hM5xfATOcXwEznF8BM5xfATOcXwEznF8BM5Fe3zi8gaAJcAGFDVxcXb2gHcD2ABgO0ArlDVd0rao9Fftfr4QMLr/eu4b5ud1W7Wv3XNf5r1/x0OL7N9W9/F9s7z9rz90mSvd5Cd/TGzfsx9B4O1K2fa39fZf/u6Wf/ifd8x6ydvNp5Pdfx8iDHXoBiJnL8wRilH/p8AuOgDt90IYJ2qLgKwrvg5EU0i0fCr6lMA9n7g5mUA1hY/Xgvg0gqPi4iqrNz3/B2q2gcAxf9nV25IRFQLVT+3X0RWAlgJAC1orfbuiKhE5R75+0WkEwCK/w+EvlBVu1W1S1W7GhG+yIOIaqvc8D8KYEXx4xUAHqnMcIioVqLhF5F7ATwL4BQR2SkiVwNYBeACEdkC4ILi50Q0iUTf86vq8kDp/LL2aPRXk/Txs9Ptue3zQ0Nl33dikWvHd155gll/9d1es37J1HA/vGfbfHPbT+hrZn3zbZ8y6y9ceItZn5GZEqzlYM/b/3ZkLoLPnLvZrA/+w2GzPllpbsQoln7+As/wI3KK4SdyiuEncorhJ3KK4SdyiuEncqq+pu6OMVpmheFhc9PM1KlmvXAwfOlpteXCV+QCAE5r3WXWBwtG+/Sg/SPeft1pZv35C//VrFutPADISvj4Uoi0pQYL9iXemx78hFk/vvCiWZ+0KnQ5Mo/8RE4x/EROMfxETjH8RE4x/EROMfxETjH8RE7Vvs9vXd4a619alwNH+vyxejXFluDuWG8sJQ3g9lcvN+vfXx4+R2HeCW+b297/xbvN+jEZe/alAuyfWUHDl+0eKNg/k8NqPz1nbDYubQUAayr4Ol6iWxrs7zvRFPZj8MhP5BTDT+QUw0/kFMNP5BTDT+QUw0/kFMNP5FTt+/yTeGnkcumI3cdvftaegrpl7nFm/ch/zQrWDjW1mduefdYNZv2nS7vN+l88c7VZb54S7sU/eeaPzW3bMnYfv+9s++l74jP1u0R39thjgrX8vkFz21ou0U1EH0EMP5FTDD+RUww/kVMMP5FTDD+RUww/kVPRPr+IrAFwCYABVV1cvO1mANcAOHqx+E2q+li1BvmejNG3LdjLPacpdn12pv1Ys374eHv58ZG2cG93+g772u/BFru+ZuA8sy5Zu1++dP7WYK1V7Hn5W+0y/vEr95j1f3rja8HazDuete+8ymK9fIt53kiFl+j+CYCLxrl9taqeXvxX/eATUUVFw6+qTwHYW4OxEFENJXnPf72IvCwia0RkRsVGREQ1UW74bwdwIoDTAfQB+EHoC0VkpYj0iEjPCNKbR4+I3q+s8Ktqv6rmVbUA4A4AS4yv7VbVLlXtaoQ9GSQR1U5Z4ReRzjGfXgZgY2WGQ0S1Ukqr714ASwHMEpGdAL4HYKmInA5AAWwHcG0Vx0hEVRANv6ouH+fmO8vam9g9b2mOzBF/6FC4aJ0DAKR7HoA1fzwAbbLn9dfIJdotewrB2rSN/ea2c+6ebdbfHDzRrE9d0mrWf9O4KFzsfNLcdl/BPgdhT36aWZ/xh8Ph4iSet980gSn9eYYfkVMMP5FTDD+RUww/kVMMP5FTDD+RU7WfuttQOBheahoAGuYcH6zlevsqPZySxVozEmkr6Vv2Mtotu98x683W8uOzw9N6A0DrizvMug4OmfXO9XZLbMvHzwjWdvyJve3CBmOKagDnt75q1u+e9aVgrXUSTyGveaNtPYFvi0d+IqcYfiKnGH4ipxh+IqcYfiKnGH4ipxh+Iqdq2+dXQHMTuObwA3K7esPF2CW9Wr1Les2+KxC9pBfvvmvfv9i/ozVnLGU9sNvcthDZd0xmyhSzft7ZrwRrDwx2mdte2LbBrK8Z+DOz3rZuU7BWvxO9J8vIRPDIT+QUw0/kFMNP5BTDT+QUw0/kFMNP5BTDT+RUTfv8IoJMS0uwXjhsTLUcE5maOzYtuI5Eeqsanh5bmuzrzvWIsaQy4r3y2PZinEdgTndeAXrKQrN+cfsvg7VLWu15DAoIP+YA8OQWY1pwACcdeNmsT1bSaDzfRiJTko/BIz+RUww/kVMMP5FTDD+RUww/kVMMP5FTDD+RU9E+v4jMA3AXgOMAFAB0q+otItIO4H4ACwBsB3CFqpoTzKsqCtYc8wlkT7L7zVKIzC9/TadZP2lNeKlr7bWXwY6dQ5CZ2W7W828N2PdvXM8fW1Mgdu149pN2L737kR+b9bkN4WW0ByPnZlz12mVm/eS/2WrW1Tj/QdNcsj0i22Evm57vN54PE1iPoJQjfw7At1X1kwA+C+A6ETkVwI0A1qnqIgDrip8T0SQRDb+q9qnqi8WP9wPYBGAOgGUA1ha/bC2AS6s1SCKqvAm95xeRBQDOAPA8gA5V7QNGf0EAsF+rEFFdKTn8IjINwEMAblBVewG392+3UkR6RKRnBNV5v09EE1dS+EWkEaPBv0dVf168uV9EOov1TgDj/hVCVbtVtUtVuxphX1xDRLUTDb+MLjF7J4BNqvrDMaVHAawofrwCwCOVHx4RVUspl/SeA+AqABtE5KXibTcBWAXgARG5GsAbAL5c0h6rtDRyfuvrZj3Wsmrtsy+FHFjaEax1rLMvPc0M25fkHvyU3WbMnhzeNwC0bA23ft76/Fxz271n2WPbcMGtZn1aJtzKA4DBQnhq8O/22lNv77l1gb3v/c+Z9ckqP2Bf6lwp0fCr6tMAQsk4v7LDIaJa4Rl+RE4x/EROMfxETjH8RE4x/EROMfxETtV2ie5qii3RvXufWZ6x+Riz3tJ3IFjb9s9t5raXnfR7sz5ceMusXzvrt2b9yUPhcxj+vO0hc9tmsZ8CWQlPtQ4Ahwr2eQLPHj42WHv8lVPNbU9+INLHl8g01VU6p6Tasm328yk/VPLZ9SYe+YmcYviJnGL4iZxi+ImcYviJnGL4iZxi+Imcqq8+f6xva4lMxVyI9EbfOcVeZrvzf3YFayd81556+8FvnGPWH7pitVmf32CP7att24K1Rmk0t42xrscHgO59i8367f/9uWDtlG+tN7fV2LkbaU6/XcVzDKJ9fOtxmcBDwiM/kVMMP5FTDD+RUww/kVMMP5FTDD+RUww/kVOiNbzmebq065mSYLZvq7+ZYs+3YeF8sz48f6ZZb9plzzWw4/LjzPqCn74RrO07y563f2i+/fv/L1f8yqyvu9i+Jj+3M3x+hDTY5yBI1h5b4fBhsz5pJTiH4HldhyHdW9IJMzzyEznF8BM5xfATOcXwEznF8BM5xfATOcXwEzkV7fOLyDwAdwE4DkABQLeq3iIiNwO4BsDRxcRvUtXHrPuanmnXzzZ8PljXXK70kU+QNNhTF1Rz3/VMGu25ArA4vCYAAGDjFrOcndsZrOVe32Hfd4oaOu1zK3J99loL2UUnmPXCG+HzH3R42NzWei4/l/s1hgql9flLmcwjB+DbqvqiiLQBWC8iTxRrq1X130rZERHVl2j4VbUPQF/x4/0isgnAnGoPjIiqa0Lv+UVkAYAzADxfvOl6EXlZRNaIyIzANitFpEdEekbUfjlDRLVTcvhFZBqAhwDcoKpDAG4HcCKA0zH6yuAH422nqt2q2qWqXY3SXIEhE1EllBR+EWnEaPDvUdWfA4Cq9qtqXlULAO4AsKR6wySiSouGX0QEwJ0ANqnqD8fcPvbPuJcB2Fj54RFRtZTy1/5zAFwFYIOIvFS87SYAy0XkdAAKYDuAa6P3pIDmq3Ppbaa11awXDh1KdP9We2Uytwk1N2J/we9eseuRy09zO3ZOcET1IdbKi8lvCU+nnlS2Y3awJv2lT9Veyl/7nwYw3k/Y7OkTUX3jGX5ETjH8RE4x/EROMfxETjH8RE4x/ERO1dcS3RHW5adJ+/gx1To/oRSvrzrLrC+88dnq7Tw2jbTYxw/JhLdX1PES3HUst6s3WFONnLcxBo/8RE4x/EROMfxETjH8RE4x/EROMfxETjH8RE7VdIluEXkbwNj5mmcB2F2zAUxMvY6tXscFcGzlquTY5qvqx0r5wpqG/0M7F+lR1a7UBmCo17HV67gAjq1caY2NL/uJnGL4iZxKO/zdKe/fUq9jq9dxARxbuVIZW6rv+YkoPWkf+YkoJamEX0QuEpHNIrJVRG5MYwwhIrJdRDaIyEsi0pPyWNaIyICIbBxzW7uIPCEiW4r/j7tMWkpju1lEdhUfu5dE5AspjW2eiPxGRDaJyCsi8s3i7ak+dsa4Unncav6yX0SyAF4FcAGAnQBeALBcVf+vpgMJEJHtALpUNfWesIicB+AAgLtUdXHxtn8BsFdVVxV/cc5Q1b+vk7HdDOBA2is3FxeU6Ry7sjSASwF8HSk+dsa4rkAKj1saR/4lALaq6jZVPQLgPgDLUhhH3VPVpwDs/cDNywCsLX68FqNPnpoLjK0uqGqfqr5Y/Hg/gKMrS6f62BnjSkUa4Z8D4M0xn+9EfS35rQAeF5H1IrIy7cGMo6O4bPrR5dPDy7ekI7pycy19YGXpunnsylnxutLSCP948zrVU8vhHFX9NICLAVxXfHlLpSlp5eZaGWdl6bpQ7orXlZZG+HcCmDfm87kAwpOS1Ziq9hb/HwDwMOpv9eH+o4ukFv8fSHk876mnlZvHW1kadfDY1dOK12mE/wUAi0RkoYg0AbgSwKMpjONDRGRq8Q8xEJGpAC5E/a0+/CiAFcWPVwB4JMWxvE+9rNwcWlkaKT929bbidSon+RRbGT8CkAWwRlW/X/NBjENETsDo0R4Yndn4Z2mOTUTuBbAUo1d99QP4HoBfAHgAwMcBvAHgy6pa8z+8Bca2FKMvXd9bufnoe+waj+1cAL8FsAFAoXjzTRh9f53aY2eMazlSeNx4hh+RUzzDj8gphp/IKYafyCmGn8gphp/IKYafyCmGn8gphp/Iqf8H2vwcmKBm2uQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(generated_images[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASKS:\n",
    "\n",
    "* Visualise images generated when you do a linear interpolation in noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading:\n",
    "\n",
    "* DCGAN\n",
    "* WGAN and WGAN-GP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_gpu_tensorflow]",
   "language": "python",
   "name": "conda-env-keras_gpu_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
